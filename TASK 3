This project utilizes the Bank Marketing Dataset, which contains information about marketing campaigns conducted by a Portuguese bank. The objective is to predict whether a client will subscribe to a term deposit based on their demographic data, campaign information, and socio-economic context.

Steps and Key Highlights
1. Data Inspection and Cleaning
The dataset was loaded and inspected to understand its structure, size, and feature types.
Duplicate entries were identified and removed to ensure data integrity.
Missing values were checked, and the dataset was confirmed to have none.
2. Data Segmentation
Features were categorized into numerical and categorical for targeted analysis.
Categorical columns represented customer attributes like job, marital status, and education, while numerical columns included age, balance, and campaign duration.
3. Exploratory Data Analysis (EDA)
Numerical Features: Histograms with density plots were generated to observe distributions and detect skewness.
Categorical Features: Count plots revealed the frequency of each category, providing insights into customer demographics.
Outlier Detection and Removal: The Interquartile Range (IQR) method was applied to handle outliers in critical features like age, campaign, and duration, ensuring a robust dataset.
4. Correlation Analysis
A heatmap was created to visualize correlations among numerical features.
Highly correlated features, such as euribor3m and nr.employed, were identified and removed to reduce multicollinearity.
5. Data Transformation
Categorical variables were encoded into numerical values using label encoding, making them suitable for machine learning algorithms.
6. Splitting the Dataset
The dataset was split into training and testing sets, with 70% used for training the model and 30% reserved for testing.
7. Building the Predictive Model
A Decision Tree Classifier was implemented, leveraging both the Gini Index and Entropy criteria to determine splits.
The model's complexity was controlled using hyperparameters like maximum depth and minimum samples per split.
8. Model Evaluation
Performance metrics, including accuracy, confusion matrix, and classification report, were used to evaluate the model.
Training and testing accuracy were compared to ensure the model was neither overfitting nor underfitting.
9. Visualizing the Decision Tree
The structure of the decision tree was visualized to interpret decision-making pathways, highlighting feature importance.
Key Outcomes
Effective data cleaning and EDA revealed patterns in customer behavior and campaign success rates.
The decision tree classifier achieved satisfactory predictive accuracy and provided interpretable results.
Insights into customer segments likely to subscribe were derived, offering actionable recommendations for marketing strategies.
This structured approach to data cleaning, EDA, and predictive modeling demonstrates how machine learning can be applied to solve real-world business challenges



import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')
%matplotlib inline

df = pd.read_csv(r"bank-additional-full.csv",delimiter=';')

df.head(5)

df.rename(columns={'y':'subscribed_deposit'}, inplace=True)

df.shape

df.columns

df.info()

df.describe()

df.isnull().sum()

df.duplicated().sum()

df.drop_duplicates(inplace=True)

df.duplicated().sum()

df_obj= df.select_dtypes(include='object').columns

df_num= df.select_dtypes(exclude='object').columns

for feature in df_num:
    sns.histplot(x=feature,data=df,bins=25,kde=True,color='#5f366e')
    plt.show()

for feature in df_obj:
    plt.figure(figsize=(8,3))
    plt.title(f"Count plot of {feature}")
    sns.countplot(x=feature,data=df,palette='viridis')
    plt.xticks(rotation=40)
    plt.show()

df.plot(kind='box', subplots=True, layout=(5,2), figsize=(10,30))
plt.show()

columns = ['age', 'campaign', 'duration']

for column in columns:
    q1 = np.percentile(df[column], 25)
    q3 = np.percentile(df[column], 75)
    iqr = q3 - q1
    lower_bound = q1 - 1.5 * iqr
    upper_bound = q3 + 1.5 * iqr
    # Filter the DataFrame for the current column
    df = df[(df[column] >= lower_bound) & (df[column] <= upper_bound)]

df.plot(kind='box', subplots=True, layout=(5,2), figsize=(10,30))
plt.show()

# Select only the numerical columns
numerical_df = df.select_dtypes(include=[np.number])

# Create the heatmap
plt.figure(figsize=(10, 8))
sns.heatmap(numerical_df.corr(), annot=True, cmap='coolwarm', vmin=-1, vmax=1)
plt.title('Correlation Matrix Heatmap')
plt.show()

high_corr_cols = ['emp.var.rate','euribor3m','nr.employed']

# copy the original dataframe

df1=df.copy()

# Removing high correlated columns from the dataset
df1.drop(high_corr_cols, inplace=True, axis=1)
df1.columns

df1.shape

from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
df_encoded = df1.apply(le.fit_transform)
df_encoded

df_encoded['subscribed_deposit'].value_counts(normalize=True)*100

## independent variables
x = df_encoded.iloc[:,:-1]   

## Target variable
y = df_encoded.iloc[:,-1]     

x.shape

y.shape

from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.3,random_state=1)
print(x_train.shape)
print(x_test.shape)
print(y_train.shape)
print(y_test.shape)

from sklearn.tree import DecisionTreeClassifier

dc = DecisionTreeClassifier(criterion='gini',max_depth=5,min_samples_split=10)
dc.fit(x_train,y_train)

print("Training accuracy:",dc.score(x_train,y_train))
print("Testing accuracy:",dc.score(x_test,y_test))

y_pred=dc.predict(x_test)

from sklearn.metrics import confusion_matrix,classification_report,accuracy_score
print(accuracy_score(y_test,y_pred))

print(confusion_matrix(y_test,y_pred))

print(classification_report(y_test,y_pred))

from sklearn.tree import plot_tree

feature_names=df.columns.tolist()
plt.figure(figsize=(40,20))
class_names=["class_0","class_1"]
plot_tree(dc, feature_names=feature_names, class_names=class_names, filled=True,fontsize=12)
plt.show()

dc1=DecisionTreeClassifier(criterion='entropy',max_depth=5,min_samples_split=10)
dc1.fit(x_train,y_train)

print("Training accuracy:",dc1.score(x_train,y_train))
print("Testing accuracy:",dc1.score(x_test,y_test))

y1_pred=dc1.predict(x_test)

print(accuracy_score(y_test,y1_pred))

print(confusion_matrix(y_test,y1_pred))

print(classification_report(y_test,y1_pred))

cn=['no','yes']
fn=x_train.columns
plt.figure(figsize=(40,20))
plot_tree(dc1, feature_names=fn.tolist(), class_names=cn, filled=True,fontsize=12)
plt.show()
